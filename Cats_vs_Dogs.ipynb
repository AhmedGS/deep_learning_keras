{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cats vs Dogs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPoOMKwT+0dOxg/RfjMAA3J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AhmedGS/deep_learning_keras/blob/master/Cats_vs_Dogs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_GVWHpOvzrz",
        "colab_type": "code",
        "outputId": "e4082a26-1bde-4797-9022-c805b23a58af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Jun  5 20:38:17 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.82       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtSX2TMp-jxn",
        "colab_type": "code",
        "outputId": "f30a55f5-2a70-4ffe-9e79-4d54d23781eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        }
      },
      "source": [
        "!pip install tf-nightly"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tf-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/50/8edc6208752c3b5858144e74905c88603c7136b615f8906b44ab083f55e0/tf_nightly-2.3.0.dev20200605-cp36-cp36m-manylinux2010_x86_64.whl (526.2MB)\n",
            "\u001b[K     |████████████████████████████████| 526.2MB 31kB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.12.1)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (2.10.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.10.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.3.3)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.29.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.1.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.12.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.2.1)\n",
            "Collecting tf-estimator-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d6/6e432b3dae22f88f0e1733899411657fea04ca27efb5bf01bc6db52a63c6/tf_estimator_nightly-2.3.0.dev2020060501-py2.py3-none-any.whl (459kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 57.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.9.0)\n",
            "Collecting tb-nightly<2.4.0a0,>=2.3.0a0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/70/ebd53fb971ef34ec427920f48648fe8619b946c0984803d25e36a6eb72ef/tb_nightly-2.3.0a20200605-py3-none-any.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 49.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.4.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.18.4)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.6.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.34.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tf-nightly) (47.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (2.23.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (1.7.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (3.2.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (0.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (1.6.0.post3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (3.0.4)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (3.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (0.2.8)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (4.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (1.6.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (3.1.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (3.1.0)\n",
            "Installing collected packages: tf-estimator-nightly, tb-nightly, tf-nightly\n",
            "Successfully installed tb-nightly-2.3.0a20200605 tf-estimator-nightly-2.3.0.dev2020060501 tf-nightly-2.3.0.dev20200605\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJib6aCE7JY6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iK5CFC48XiT",
        "colab_type": "code",
        "outputId": "5d2ab1c3-0433-4591-d9de-eb7816f026e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!curl -O https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  786M  100  786M    0     0  51.7M      0  0:00:15  0:00:15 --:--:-- 26.8M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HQeIyp38d0h",
        "colab_type": "code",
        "outputId": "35b6f4b4-28f1-4277-eaf8-773c8a5c7969",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!unzip -q kagglecatsanddogs_3367a.zip\n",
        "!ls"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " kagglecatsanddogs_3367a.zip   PetImages        sample_data\n",
            "'MSR-LA - 3467.docx'\t      'readme[1].txt'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0hosDZ38mzD",
        "colab_type": "code",
        "outputId": "267f604c-f45a-4880-a1f5-e20558aae599",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# !rm kagglecatsanddogs_3367a.zip\n",
        "!ls PetImages"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cat  Dog\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9soaWhpsC1W",
        "colab_type": "code",
        "outputId": "e5914858-6b22-4fb6-cfd4-a04c0f8dcf7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "\n",
        "num_skipped = 0\n",
        "for folder_name in (\"Cat\", \"Dog\"):\n",
        "    folder_path = os.path.join(\"PetImages\", folder_name)\n",
        "    for fname in os.listdir(folder_path):\n",
        "        fpath = os.path.join(folder_path, fname)\n",
        "        try:\n",
        "            fobj = open(fpath, \"rb\")\n",
        "            is_jfif = tf.compat.as_bytes(\"JFIF\") in fobj.peek(10)\n",
        "        finally:\n",
        "            fobj.close()\n",
        "\n",
        "        if not is_jfif:\n",
        "            num_skipped += 1\n",
        "            # Delete corrupted image\n",
        "            os.remove(fpath)\n",
        "\n",
        "print(\"Deleted %d images\" % num_skipped)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Deleted 0 images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iS2ygrxw7W2A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model:\n",
        "\n",
        "  def __init__(self, data_path, n_classes=2, image_size=(180, 180), batch_size=32):\n",
        "    self.data_path = data_path\n",
        "    self.image_size = image_size\n",
        "    self.batch_size = batch_size\n",
        "    self.get_data()\n",
        "    # self.data_augmentation()\n",
        "    self.make_model(n_classes)\n",
        "\n",
        "  def get_data(self, rand_seed=1337):\n",
        "    self.train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    \"PetImages\",\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=1337,\n",
        "    image_size=self.image_size,\n",
        "    batch_size=self.batch_size,\n",
        "    )\n",
        "    self.val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    \"PetImages\",\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=1337,\n",
        "    image_size=self.image_size,\n",
        "    batch_size=self.batch_size,\n",
        "    )\n",
        "\n",
        "  # def get_data_generators(self, rand_seed=1337):\n",
        "  #   self.train_generator = self.train_datagen.flow_from_directory(\n",
        "  #       self.data_path,  # This is the source directory for training images\n",
        "  #       target_size=self.image_size,  # All images will be resized to 200 x 200\n",
        "  #       batch_size=self.batch_size,\n",
        "  #       # Specify the classes explicitly\n",
        "  #       classes = ['Cat','Dog'],\n",
        "  #       subset=\"training\",\n",
        "  #       shuffle=True,\n",
        "  #       class_mode='binary')\n",
        "    \n",
        "  #   self.val_generator = self.val_datagen.flow_from_directory(\n",
        "  #       self.data_path,\n",
        "  #       target_size=self.image_size,  # All images will be resized to 200 x 200\n",
        "  #       batch_size=self.batch_size,\n",
        "  #       # Specify the classes explicitly\n",
        "  #       classes = ['Cat','Dog'],\n",
        "  #       subset=\"validation\",\n",
        "  #       shuffle=True,\n",
        "  #       class_mode='binary'\n",
        "  #   )\n",
        "\n",
        "  def data_augmentation(self):\n",
        "\n",
        "    self.train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        # rotation_range=30,\n",
        "        # width_shift_range=0.2,\n",
        "        # # height_shift_range=0.2,\n",
        "        # shear_range=0.2,\n",
        "        # zoom_range=0.2,\n",
        "        # horizontal_flip=True,\n",
        "        validation_split=0.2,\n",
        "        # fill_mode='nearest'\n",
        "        )\n",
        "    self.val_datagen = ImageDataGenerator(\n",
        "        rescale=1./255\n",
        "        )\n",
        "\n",
        "\n",
        "  def evaluate_model(self):\n",
        "    print(self.model.evaluate(self.val_ds))\n",
        "\n",
        "  def make_model(self, num_classes=2):\n",
        "\n",
        "    data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
        "        layers.experimental.preprocessing.RandomRotation(0.1),\n",
        "    ]\n",
        "    )\n",
        "    \n",
        "    input_shape = self.image_size + (3,)\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    # Image augmentation block\n",
        "    x = data_augmentation(inputs)\n",
        "    # x = inputs\n",
        "\n",
        "    # Entry block\n",
        "    x = layers.experimental.preprocessing.Rescaling(1.0 / 255)(x)\n",
        "    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "    x = layers.Conv2D(64, 3, padding=\"same\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "    previous_block_activation = x  # Set aside residual\n",
        "\n",
        "    for size in [128, 256, 512, 728]:\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
        "\n",
        "        # Project residual\n",
        "        residual = layers.Conv2D(size, 1, strides=2, padding=\"same\")(\n",
        "            previous_block_activation\n",
        "        )\n",
        "        x = layers.add([x, residual])  # Add back residual\n",
        "        previous_block_activation = x  # Set aside next residual\n",
        "\n",
        "    x = layers.SeparableConv2D(1024, 3, padding=\"same\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    if num_classes == 2:\n",
        "        activation = \"sigmoid\"\n",
        "        units = 1\n",
        "    else:\n",
        "        activation = \"softmax\"\n",
        "        units = num_classes\n",
        "\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(units, activation=activation)(x)\n",
        "    self.model = keras.Model(inputs, outputs)\n",
        "\n",
        "\n",
        "  def train(self, n_epoch=50):\n",
        "\n",
        "    self.model.compile(\n",
        "      optimizer=keras.optimizers.RMSprop(lr=0.001),\n",
        "      loss=\"binary_crossentropy\",\n",
        "      metrics=[\"accuracy\"],\n",
        "      )\n",
        "    \n",
        "    total_sample = 23410 * 0.8\n",
        "    # callbacks = [keras.callbacks.ModelCheckpoint(\"save_at_{epoch}.h5\"),]\n",
        "\n",
        "    self.history = self.model.fit(\n",
        "        self.train_ds, \n",
        "        steps_per_epoch=int(total_sample/self.batch_size),  \n",
        "        epochs=n_epoch,\n",
        "        # callbacks=callbacks,\n",
        "        validation_data=self.val_ds,\n",
        "        verbose=1)\n",
        "    \n",
        "   \n",
        "\n",
        "    \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOktjL5F9KSJ",
        "colab_type": "code",
        "outputId": "56e637d6-e5fa-4223-c9c4-6eef18b34641",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "kmodel = Model('PetImages')\n",
        "# kmodel.get_data()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 23410 files belonging to 2 classes.\n",
            "Using 18728 files for training.\n",
            "Found 23410 files belonging to 2 classes.\n",
            "Using 4682 files for validation.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfkczuPQn5y7",
        "colab_type": "code",
        "outputId": "2ac4eaca-fede-4cd1-90b6-d04c07d26b6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "kmodel.train(n_epoch=1)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "585/585 [==============================] - 144s 246ms/step - loss: 0.6640 - accuracy: 0.6330 - val_loss: 0.6046 - val_accuracy: 0.6805\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyeCuKSDy9Hh",
        "colab_type": "code",
        "outputId": "c3f7715e-e051-45e5-dcfa-32715ef856cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "kmodel.evaluate_model()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "147/147 [==============================] - 11s 77ms/step - loss: 0.6046 - accuracy: 0.6805\n",
            "[0.6046444773674011, 0.6804784536361694]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQDbfwATuyxM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# kmodel.model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJ_hAmjzONhR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flDfetd8ONR-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cA6Zs57eONM0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-8Vbc2tONFh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raiY3BLrOM9c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNVkacPq94EQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# plt.figure(figsize=(10, 10))\n",
        "# for images, labels in kmodel.train_ds.take(1):\n",
        "#     for i in range(9):\n",
        "#         ax = plt.subplot(3, 3, i + 1)\n",
        "#         plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "#         plt.title(int(labels[i]))\n",
        "#         plt.axis(\"off\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNjjWK9BmeGc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}